# -*- coding: utf-8 -*-
"""Copy of tweeter_entailment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eZ8-gc7EaTe65oBz_zHXTZEdA4OIr-CZ
"""

! pip install kaggle

! mkdir -p ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

from zipfile import ZipFile

dataset = '/content/dataset.zip'

with ZipFile (dataset, 'r') as zip:
  zip.extractall()
  print('extracted successfully')

"""Dependencies"""

import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

"""Remove stopwords"""

print(stopwords.words('english'))

"""Data Processing"""

tweeterData = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding= 'ISO-8859-1')



tweeterData.head()

fieldName = ['Target', 'ID', 'Date', 'Flag', 'User', 'Text']

tweeterData = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names=fieldName, encoding= 'ISO-8859-1')

tweeterData.head()

"""check missed value"""

tweeterData.isnull().sum()

"""so there is no any missing value and then no need to process

Checking columns
"""

tweeterData['Target'].value_counts()

"""Replace the lable 4 to 1"""

tweeterData.replace({'Target' : {4:1}}, inplace= True)

tweeterData['Target'].value_counts()

"""**0** means Negative and **1** means Positive tweets

**Stemming**
"""

port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)
  return stemmed_content

tweeterData['stemmedText'] = tweeterData['Text'].apply(stemming)

tweeterData.head()

tweeterData['Target'].value_counts()

"""Separating lables from Dataset

then splitting the data to training and testing

***from sklearn.model_selection import train_test_split***
"""

x = tweeterData['stemmedText'].values
y = tweeterData['Target'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)

print(x.shape, x_train.shape, x_test.shape)

"""In this sstep we are going to converting the textual data to the numerical

***from sklearn.feature_extraction.text import TfidfVectorizer***
"""

vectorizer = TfidfVectorizer()

x_train = vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)

print(x_train, x_test)

"""Training the model (Logistic Regression)

***from sklearn.linear_model import LogisticRegression***
"""

LogisticRegressionModel = LogisticRegression()

LogisticRegressionModel.fit(x_train, y_train)

"""Evaluation with Accurace Score

***from sklearn.metrics import accuracy_score***
"""

x_train_prediction = LogisticRegressionModel.predict(x_train)
train_data_accuracy = accuracy_score(y_train, x_train_prediction)

print('on the training data: ' , train_data_accuracy)

x_test_prediction = LogisticRegressionModel.predict(x_test)
test_data_accuracy = accuracy_score(y_test, x_test_prediction)

print('on the testing data: ' , test_data_accuracy)

import pickle

fileName = 'LogisticRegressionModel.sav'
pickle.dump(LogisticRegressionModel, open(fileName, 'wb'))

newModel = pickle.load(open('/content/LogisticRegressionModel.sav', 'rb'))

x_new = x_test[225]

prediction = newModel.predict(x_new)
print(prediction)

if (prediction[0]==0):
  print('Negative')
else:
  print('Positive')

x_new = x_test[1225]

prediction = newModel.predict(x_new)
print(prediction)

if (prediction[0]==0):
  print('Negative')
else:
  print('Positive')
